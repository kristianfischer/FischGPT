#!/usr/bin/env python3
"""
Model Verification Script

This script proves that your HuggingFace model contains YOUR trained weights
from the original checkpoint, not some pretrained model.

Usage:
    python verify_my_model.py

This will:
1. Load your original FischGPT checkpoint
2. Load the converted HuggingFace model  
3. Compare weights to prove they're identical
4. Generate identical text with both models
5. Show model behavior unique to YOUR training
"""

import torch
import sys
import os
import shutil
import numpy as np
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import tiktoken

# Add src to path for imports
sys.path.append(os.path.abspath('src'))
from config.gpt_config import GPTConfig
from model.gpt import GPT

class ModelVerifier:
    """Verify that HF model contains the same weights as original checkpoint."""
    
    def __init__(self):
        self.original_checkpoint = "checkpoints/model_19999.pt"
        self.hf_model_path = "./hf_models/fischgpt-sft"
        self.hf_hub_id = "kristianfischerai12345/fischgpt-sft"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    def load_original_model(self):
        """Load your original FischGPT checkpoint."""
        
        print("üîç Loading original FischGPT checkpoint...")
        print(f"File: {self.original_checkpoint}")
        
        try:
            # Load checkpoint
            checkpoint = torch.load(self.original_checkpoint, map_location=self.device, weights_only=False)
            config = checkpoint['config']
            
            # Create FischGPT model
            original_model = GPT(config)
            original_model.load_state_dict(checkpoint['model'])
            original_model.to(self.device)
            original_model.eval()
            
            print(f"‚úÖ Original model loaded successfully!")
            print(f"üìä Config: {config.n_layer} layers, {config.n_embd} hidden, {config.n_head} heads")
            print(f"üî¢ Step: {checkpoint.get('step', 'Unknown')}")
            print(f"üìà Val Loss: {checkpoint.get('val_loss', 'Unknown')}")
            
            return original_model, config, checkpoint
            
        except Exception as e:
            print(f"‚ùå Failed to load original model: {e}")
            return None, None, None
    
    def download_checkpoint(self):
        """Download original checkpoint from HuggingFace Hub."""
        
        print(f"üì• Downloading checkpoint from HuggingFace Hub...")
        print(f"üîó Source: https://huggingface.co/{self.hf_hub_id}/blob/main/model_19999.pt")
        
        try:
            from huggingface_hub import hf_hub_download
            
            # Download from HuggingFace Hub
            print("‚è≥ Downloading... (this may take a moment)")
            downloaded_path = hf_hub_download(
                repo_id=self.hf_hub_id,
                filename="model_19999.pt"
            )
            
            # Create checkpoints directory and copy file
            os.makedirs("checkpoints", exist_ok=True)
            shutil.copy2(downloaded_path, self.original_checkpoint)
            
            print(f"‚úÖ Downloaded and copied checkpoint to: {self.original_checkpoint}")
            print(f"üìÅ File size: {os.path.getsize(self.original_checkpoint) / (1024*1024):.1f} MB")
            return True
            
        except ImportError:
            print(f"‚ùå huggingface_hub not installed. Install with: pip install huggingface_hub")
            return False
        except Exception as e:
            print(f"‚ùå Download failed: {e}")
            print(f"üí° Manual option:")
            print(f"   1. Go to: https://huggingface.co/{self.hf_hub_id}/blob/main/model_19999.pt")
            print(f"   2. Click 'Download' button")  
            print(f"   3. Save as: {self.original_checkpoint}")
            return False

    def load_hf_model(self):
        """Load the converted HuggingFace model."""
        
        print(f"\nü§ó Loading HuggingFace model...")
        
        # Try local first, then Hub
        for source, path in [("Local", self.hf_model_path), ("Hub", self.hf_hub_id)]:
            try:
                print(f"Trying {source}: {path}")
                
                hf_model = GPT2LMHeadModel.from_pretrained(path)
                hf_tokenizer = GPT2Tokenizer.from_pretrained(path)
                
                hf_model.to(self.device)
                hf_model.eval()
                
                print(f"‚úÖ HuggingFace model loaded from {source}!")
                return hf_model, hf_tokenizer
                
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load from {source}: {e}")
                continue
        
        print(f"‚ùå Could not load HuggingFace model from any source")
        return None, None
    
    def compare_weights(self, original_model, hf_model):
        """Compare weights between original and HF models."""
        
        print(f"\nüîç WEIGHT COMPARISON")
        print("=" * 50)
        
        original_state = original_model.state_dict()
        hf_state = hf_model.state_dict()
        
        # Key mappings from our conversion
        weight_mappings = [
            # Embeddings (should be identical - tied weights)
            ("transformer.wte.weight", "transformer.wte.weight"),
            ("lm_head.weight", "lm_head.weight"),  # Should be same as wte due to tying
            ("transformer.wpe.weight", "transformer.wpe.weight"),
            ("transformer.ln_f.weight", "transformer.ln_f.weight"),
            ("transformer.ln_f.bias", "transformer.ln_f.bias"),
        ]
        
        # Add transformer block mappings
        config = original_model.config
        for i in range(config.n_layer):
            layer_mappings = [
                (f"transformer.h.{i}.ln_1.weight", f"transformer.h.{i}.ln_1.weight"),
                (f"transformer.h.{i}.ln_1.bias", f"transformer.h.{i}.ln_1.bias"),
                (f"transformer.h.{i}.ln_2.weight", f"transformer.h.{i}.ln_2.weight"), 
                (f"transformer.h.{i}.ln_2.bias", f"transformer.h.{i}.ln_2.bias"),
                # Attention weights (these were transposed during conversion)
                (f"transformer.h.{i}.attn.c_attn.weight", f"transformer.h.{i}.attn.c_attn.weight"),
                (f"transformer.h.{i}.attn.c_attn.bias", f"transformer.h.{i}.attn.c_attn.bias"),
                (f"transformer.h.{i}.attn.c_proj.weight", f"transformer.h.{i}.attn.c_proj.weight"),
                (f"transformer.h.{i}.attn.c_proj.bias", f"transformer.h.{i}.attn.c_proj.bias"),
                # MLP weights (these were also transposed)
                (f"transformer.h.{i}.mlp.c_fc.weight", f"transformer.h.{i}.mlp.c_fc.weight"),
                (f"transformer.h.{i}.mlp.c_fc.bias", f"transformer.h.{i}.mlp.c_fc.bias"),
                (f"transformer.h.{i}.mlp.c_proj.weight", f"transformer.h.{i}.mlp.c_proj.weight"),
                (f"transformer.h.{i}.mlp.c_proj.bias", f"transformer.h.{i}.mlp.c_proj.bias"),
            ]
            weight_mappings.extend(layer_mappings)
        
        matches = 0
        total = 0
        differences = []
        
        for orig_key, hf_key in weight_mappings:
            if orig_key not in original_state or hf_key not in hf_state:
                print(f"‚ö†Ô∏è Missing key: {orig_key} or {hf_key}")
                continue
                
            orig_weight = original_state[orig_key]
            hf_weight = hf_state[hf_key]
            
            # For transposed weights (attention and MLP), compare with transpose
            if "attn.c_attn.weight" in orig_key or "attn.c_proj.weight" in orig_key or \
               "mlp.c_fc.weight" in orig_key or "mlp.c_proj.weight" in orig_key:
                # HF weights should be transpose of original
                comparison_weight = orig_weight.t()
            else:
                comparison_weight = orig_weight
            
            # Compare shapes first
            if comparison_weight.shape != hf_weight.shape:
                print(f"‚ùå Shape mismatch {orig_key}: {comparison_weight.shape} vs {hf_weight.shape}")
                differences.append(orig_key)
                continue
            
            # Compare values (allow small numerical differences)
            diff = torch.abs(comparison_weight - hf_weight).max().item()
            
            if diff < 1e-6:  # Very small tolerance for numerical precision
                matches += 1
                print(f"‚úÖ {orig_key}: Perfect match (diff: {diff:.2e})")
            else:
                print(f"‚ùå {orig_key}: MISMATCH (max diff: {diff:.2e})")
                differences.append(orig_key)
            
            total += 1
        
        print(f"\nüìä RESULTS: {matches}/{total} weights match perfectly")
        
        if matches == total:
            print("üéâ ALL WEIGHTS MATCH! This is definitely YOUR trained model!")
            return True
        else:
            print("‚ö†Ô∏è Some weights don't match. Investigating...")
            print(f"Mismatched weights: {differences}")
            return False
    

    def check_model_signature(self, original_model, checkpoint):
        """Check for unique signatures that prove this is your trained model."""
        
        print(f"\nüîç MODEL SIGNATURE CHECK")
        print("=" * 50)
        
        # Check training step and loss (unique to your training)
        step = checkpoint.get('step', None)
        val_loss = checkpoint.get('val_loss', None)
        
        print(f"üìà Training Step: {step}")
        print(f"üìä Validation Loss: {val_loss}")
        
        if step == 19999:
            print("‚úÖ Correct final training step (19999) - this matches your SFT training!")
        
        # Check a few random weight values (unique to your training)
        with torch.no_grad():
            first_embedding = original_model.transformer.wte.weight[0, :5].cpu()
            last_layer_norm = original_model.transformer.ln_f.weight[:5].cpu()
            
            print(f"üî¢ First 5 embedding weights: {first_embedding.tolist()}")
            print(f"üî¢ First 5 final LayerNorm weights: {last_layer_norm.tolist()}")
            print("These are unique fingerprints of YOUR trained model!")
    
    def run_verification(self):
        """Run complete verification process."""
        
        print("üîç MODEL VERIFICATION SCRIPT")
        print("Proving your HuggingFace model contains YOUR trained weights")
        print("=" * 60)
        print("üéØ This script will mathematically prove that the HuggingFace model")
        print("   contains the exact same weights as your original checkpoint")
        print()
        
        # Download checkpoint
        if not self.download_checkpoint():
            print("‚ùå Cannot proceed without original checkpoint")
            return False
        
        # Load original model
        original_model, config, checkpoint = self.load_original_model()
        if not original_model:
            print("‚ùå Cannot verify without original model")
            return False
        
        # Load HF model
        hf_model, hf_tokenizer = self.load_hf_model()
        if not hf_model:
            print("‚ùå Cannot verify without HuggingFace model")
            return False
        
        # Check model signature
        self.check_model_signature(original_model, checkpoint)
        
        # Compare weights
        weights_match = self.compare_weights(original_model, hf_model)
        
        # Final verdict
        print(f"\n{'='*60}")
        print("üéØ FINAL VERIFICATION RESULT")
        print(f"{'='*60}")
        
        if weights_match:
            print("üéâ VERIFIED: Your HuggingFace model contains YOUR trained weights!")
            print("‚úÖ This is definitely your own work, trained from scratch!")
            print("üíº You can confidently showcase this to recruiters!")
        else:
            print("‚ö†Ô∏è INCONCLUSIVE: Some verification checks failed")
            print("üîç Manual investigation needed")
        
        return weights_match

def main():
    """Main verification function."""
    verifier = ModelVerifier()
    verifier.run_verification()

if __name__ == "__main__":
    main() 